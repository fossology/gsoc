---
title: Week 2
author: Shreya Gautam
tags: [gsoc24, pipeline]
---

<!--
SPDX-License-Identifier: CC-BY-SA-4.0

SPDX-FileCopyrightText: 2024 Shreya Gautam <gautamm.shreya@gmail.com>
-->

# WEEK 2
*(June 6, 2024)*

## Attendees:
- [Kaushlendra Pratap](https://github.com/Kaushl2208)
- [Anupam Ghosh](https://github.com/ag4ums)
- [Gaurav Mishra](https://github.com/GMishx)
- [Shaheem Azmal M MD](https://github.com/shaheemazmalmmd)
- [Ayush Bhardwaj](https://github.com/hastagAB)
- [Avinal Kumar](https://github.com/avinal)

## Discussion:
* I presented a detailed plan via [PowerPoint](https://drive.google.com/file/d/1OFfabDOyZdiO0U_pBJi4SgcXKgDh6Jdh/view?usp=sharing) outlining my approach to the project. This included an explanation of Safaa's current functionality and its key features, along with identified areas for enhancing pipeline efficiency.
* I initiated a discussion comparing TF-IDF with BERT/GPT embeddings. Given our relatively small dataset (~20k), I proposed transitioning to a transformer model if we scale up our dataset size.
* Additionally, I recommended replacing the current SVM model with either a transformer model or LSTM for improved performance.
* I also suggested initiating parallel work on developing a Python library for the NER-POS tagging task.
* To guide me in formulating a structured approach to the pipeline steps, [Kaushal](https://github.com/Kaushl2208) shared his perspective by presenting his envisioned pipeline for the project. His insights proved invaluable in identifying and comprehending critical aspects of the pipeline's development and implementation.

### Engagements
Further discussions covered aspects of model development, validation, testing, monitoring, and ongoing maintenance. I participated in the general meeting, providing comprehensive updates on project progress.

## Subsequent Steps
* I was assigned the task of writing a script to connect to the Fossology database and retrieve copyright contents. Initially, I was instructed to test the script on my locally hosted Fossology instance using dummy data. Once validated, I will proceed to use the script to fetch actual data.
* Additionally, I was tasked with creating a MVP to substantiate my proposal for using embedding models and transformers instead of TF-IDF and SVM. This prototype aims to verify their effectiveness and potential for significant improvements.
